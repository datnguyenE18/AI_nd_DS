{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yH_fJcBbt_6m"
   },
   "outputs": [],
   "source": [
    "# Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "danTUjPtRBar"
   },
   "source": [
    "# Bài tập lập trình: Python trong Machine Learning / Deep Learning\n",
    "\n",
    "Trong bài tập lập trình này, các bạn sẽ ôn lại các chức năng cơ bản về toán học và thư viện Numpy. \n",
    "\n",
    "**Một số chú ý:**\n",
    "- Trong bài tập này bạn sẽ sử dụng Python 3.\n",
    "- Cố gắng không sử dụng các vòng lặp (for, while). \n",
    "- Không chỉnh sửa các tên hàm. \n",
    "- Sau khi bạn viết Code của mình xong, hãy chạy dòng Code đó để xem kết quả bên dưới. \n",
    "\n",
    "**Sau bài tập này bạn sẽ:**\n",
    "- Có thể sử dụng các hàm trong thư viện `Numpy` và các phép tính numpy matrix/vector.\n",
    "- Phân biệt được sự khác nhau giữa thư viện `math` và thư viện `numpy`.\n",
    "- Kỹ thuật \"Broadcasting\".\n",
    "- Thực hiện tính toán trên các Vector.\n",
    "\n",
    "Hãy bắt đầu nào!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uA9cap6RBas"
   },
   "source": [
    "## Hướng dẫn làm bài ##\n",
    "\n",
    "Các bạn sẽ bắt đầu Code trong phần `### START CODE HERE ###` và `### END CODE HERE ###`. Các bạn hãy nhớ đừng sửa bất kỳ tên hàm nào ngoài phần Code này. \n",
    "\n",
    "Sau khi viết xong Code của bạn, bạn hãy ấn \"SHIFT\"+\"ENTER\" để thực hiện chạy lệnh của Cell đó. Sau đó kiểm tra với kết quả có sẵn bên dưới để xem đã thực hiện đúng hay sai, nếu sai hãy thử lại !\n",
    "\n",
    "Trong phần Code: các bạn hãy cố gắng thực hiện ít dòng Code nhất theo chỉ định \"(≈ X lines of code)\". Mặc dù đây không phải là hạn chế về số dòng Code của bạn, nhưng hãy tối ưu sao cho ít nhất có thể."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqKnOkByRBa2"
   },
   "source": [
    "<font color='blue'>\n",
    "**Một số điều bạn nên nhớ**:\n",
    "    \n",
    "- Chạy `Cells` bằng SHIFT + ENTER (hoặc nhấn \"Run Cell\")\n",
    "- Chỉ sử dụng Python 3 để code\n",
    "- Không chỉnh sửa bất cứ thứ gì bên ngoài hàm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2rYvWg9jeb6"
   },
   "source": [
    "## 1) Sử dụng Numpy xây dựng 1 số hàm cơ bản ##\n",
    "\n",
    "- Trong bài tập này, bạn sẽ thực hiện 1 vài hàm từ thư viện numpy như: np.exp, np.log, np.reshape,... \n",
    "\n",
    "(Truy cập vào trang chủ [numpy](www.numpy.org) để xem chi tiết hơn.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pofoxDHmRBa3"
   },
   "source": [
    "\n",
    "\n",
    "### 1.1 - sigmoid function, np.exp() ###\n",
    "Trước khi sử dụng np.exp(), bạn sẽ sử dụng hàm math.exp() để thực hiện hàm `Sigmoid`. Bạn sẽ thấy lý do tại sao chúng ta sử dụng np.exp() thay cho math.exp()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3QP3-JsF1gK"
   },
   "source": [
    "**Bài tập 1.1**: Thực hiện xây dựng hàm `Sigmoid` với số thực x. Sử dụng `math.exp()` cho phần hàm này.\n",
    "\n",
    "**Nhắc lại**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ : là hàm phi tuyến được sử dụng rộng rãi trong Machine Learning.\n",
    "\n",
    "![](https://imgur.com/MyypY7K.png)\n",
    "<!-- <img src=\"https://imgur.com/MyypY7K.png\" style=\"width:20%;height:50%;\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nnmuS1NrRBa3"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar \n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAmC4p8KRBa7",
    "outputId": "c2f7ce93-dde3-456e-d4bb-bcd97d3b4b66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999546021312976"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0SjDPG_RBa-"
   },
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "```python\n",
    "0.9999546021312976\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ouXCOkpRBa-"
   },
   "source": [
    "Trong thực tế, chúng ta không sử dụng thư viện `math` trong Machine Learning / Deep Learning (ML/DL) bởi vì đầu vào của hàm này là số thực.\n",
    "Trong ML/DL đầu vào của chúng ta thường là matrices và vectors. Đó chính là lý do `numpy` hữu ích hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "VkJGepRNRBa_",
    "outputId": "84acbeea-6511-4d18-8db2-f86854f80491"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1e24019d84ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Khi chạy Cell này: các bạn sẽ thấy lỗi vì x là vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbasic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-36d47ba19f31>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "### Lý do mà chúng ta sử dung 'numpy' thay cho 'math' trong ML/DL ###\n",
    "x = [1, 2, 3]\n",
    "\n",
    "# Khi chạy Cell này: các bạn sẽ thấy lỗi vì x là vector.\n",
    "basic_sigmoid(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTcwn8kiRBbD"
   },
   "source": [
    "Nếu ta có $ x = (x_1, x_2, ..., x_n)$ là 1 vector hàng thì $np.exp(x)$ sẽ thực hiện tính toán trên từng phần tử của x. Vì vậy, chúng ta sẽ có kết quả như sau: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muauSoFLRBbE",
    "outputId": "c7c73ddb-a1a9-42fc-87b2-92f51e0bd3e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saegMcvvRBbH"
   },
   "source": [
    "Hơn nữa, nếu x là 1 vector, thì khi thực hiện $s = x + 3$ hay $s = \\frac{1}{x}$ sẽ cho đầu ra cùng kích thước với x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WjWDYwdRBbI",
    "outputId": "95cf8080-216d-4550-c0d6-21a08c4082e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + 3: [4 5 6]\n",
      "1 / x: [1.         0.5        0.33333333]\n"
     ]
    }
   ],
   "source": [
    "# example of vector operation\n",
    "x = np.array([1, 2, 3])\n",
    "print('x + 3:', x + 3)\n",
    "print('1 / x:', 1/x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNGJtOYvRBbM"
   },
   "source": [
    "Các hàm trong thư viện Numpy tương đối là nhiều và rất hữu ích, nếu bạn cần sử dụng hàm nào hãy lên trang tài liệu chính thức của Nympy [Xem thêm tại đây](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "**Bài tập 1.2**: Thực hiện hàm `Sigmoid` sử dụng `numpy`.\n",
    "\n",
    "**Hướng dẫn**: bây giờ, x có thể là 1 vô hướng, 1 vector hay 1 matrix. Cấu trúc của dạng này là các (vectors, matrices...) được gọi là các numpy array. \n",
    "$$ \\text{Với } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "s4nMRYHoRBbM"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYKFxilgRBbQ",
    "outputId": "61eea0d9-80ed-4bfc-a505-82c88bb2b0a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMBgm8WtRBbT"
   },
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "\n",
    "```python\n",
    "array([0.73105858, 0.88079708, 0.95257413])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-W9alhoRBbV"
   },
   "source": [
    "### 1.2 - Đạo hàm của hàm Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OweMhL9RF9rf"
   },
   "source": [
    "**Bài tập 2**: Thực hiện hàm sigmoid_grad() để tính đạo hàm của hàm Sigmoid với đầu vào là x. Công thức toán học: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "**Hướng dẫn**:\n",
    "Bạn sẽ thực hiện 2 bước trong phần này:\n",
    "1. Thực hiện gọi hàm sigmoid với đầu vào là x (Hàm Sigmoid sử dụng `numpy` mà các bạn đã viết ở trên).\n",
    "2. Tính $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "83DdD5cERBbV"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = sigmoid(x)\n",
    "    ds = s*(1 - s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gMO2UK0qRBbY",
    "outputId": "e21f413c-2045-4ca8-c89f-978f34b7e219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXxOlJ9JRBbb"
   },
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "```python\n",
    "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPMsNtE5RBbc"
   },
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Hai hàm được sử dụng phổ biến trong ML/DL là [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) và [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape: đưa ra số chiều của matrix/vector X.\n",
    "- X.reshape(...): định hình lại X sang một chiều khác.\n",
    "\n",
    "**Câu hỏi phụ**: Các bạn hãy phân biệt sự khác nhau giữa `Transpose` và `Reshape`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKSAqq5izC5F"
   },
   "source": [
    "**Ans:** Phân biệt `Transpose` và `Reshape`:\n",
    "\n",
    "+) `Reshape` nhận một ma trận với hình dạng đầu vào và định dạng lại ma trận đó sao cho số lượng hàng và cột thay đổi; nhưng bố cục, thứ tự dữ liệu thì không đổi. Hiểu đơn giản thì `Reshape` làm thẳng ma trận rồi cắt ra thành các hàng theo hình dạng mới. \n",
    "\n",
    "+) `Transpose` thì thay đổi bố cục, thứ tự dữ liệu ban đầu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mebzuT6TRBbd",
    "outputId": "2a3bfd0e-4f7f-4f22-a39a-e8b9354de506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "Transpose: \n",
      " [[1 3 5]\n",
      " [2 4 6]]\n",
      "Reshape: \n",
      " [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "print('Matrix A:\\n', A)\n",
    "print('Transpose: \\n', A.T)\n",
    "print('Reshape: \\n', A.reshape(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4REbU5TGRBbg"
   },
   "source": [
    "Trong Computer Science, xét ảnh RGB được đại diện bởi 1 matrix 3 chiều có shape $(length, height, depth = 3)$\n",
    "\n",
    "Tuy nhiên, khi bạn thực hiện đưa ảnh vào 1 thuật toán bạn cần chuyển nó thành vector có shape $(length*height*3, 1)$. Nói cách khác là bạn **\"unroll\"**, hay reshape 3D matrix thành 1D vector.\n",
    "\n",
    "![alt text](https://imgur.com/6ySOTB1.png)\n",
    "<!-- <img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\"> -->\n",
    "\n",
    "**Bài tập 3**: Thực hiện hàm `image2vector()` với đầu vào có shape(length, height, 3) và đầu ra là vector có (length\\*height\\*3, 1). \n",
    "\n",
    "Ví dụ, bạn có 1 array v có shape(a, b, c) -> chúng ta mong muốn đầu ra là 1 vector có shape (a*b, c).\n",
    "\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1u89Aj4cRBbh"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2], 1))\n",
    "    # hoặc: v = image.reshape((-1, 1)) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQsIJDeiRBbj",
    "outputId": "4f6918b8-ce25-4211-cf76-e6df252415ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaPTVq1tRBbm"
   },
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "```python\n",
    "image2vector(image) = [[0.67826139]\n",
    " [0.29380381]\n",
    " [0.90714982]\n",
    " [0.52835647]\n",
    " [0.4215251 ]\n",
    " [0.45017551]\n",
    " [0.92814219]\n",
    " [0.96677647]\n",
    " [0.85304703]\n",
    " [0.52351845]\n",
    " [0.19981397]\n",
    " [0.27417313]\n",
    " [0.60659855]\n",
    " [0.00533165]\n",
    " [0.10820313]\n",
    " [0.49978937]\n",
    " [0.34144279]\n",
    " [0.94630077]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g33t0ttdRBbn"
   },
   "source": [
    "### 1.4 - Normalizing rows\n",
    "\n",
    "Một kỹ thuật phổ biến khác trong ML / DL là chuẩn hoá dữ liệu. Sau khi chuẩn hoá, chúng ta sẽ có hiệu suất tốt hơn vì Gradient Descent hội tụ nhanh hơn. Ở phần này, chúng ta sẽ chuẩn hoá x thành $ \\frac{x}{\\| x\\|} $ (Chia mỗi vector của x cho Norm tương ứng của nó). \n",
    "\n",
    "Ví dụ, với $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ và $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$thì ta có        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "Lưu ý: bạn có thể nhân, chia ma trận có kích thước khác nhau và nó vẫn hoạt động tốt. Đây là kỹ thuật \"broadcasting\" mà sẽ được tìm hiểu trong phần 1.5. Hoặc bạn có thể xem thêm tại [đây](https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm?fbclid=IwAR0mN3dYIvickJiDwOU7ayjBk5JKNo9wbZWGsyDWMQVA-eQ-Y__3Q4dD-kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xf7-q1bbFpVQ"
   },
   "source": [
    "**Bài tập 4**: Thực hiện hàm normalizeRows() để chuẩn hoá theo hàng của matrix. Sau khi thực hiện hàm này với đầu vào là matrix x bạn sẽ có đầu ra là vector độ dài (có nghĩa là length của vector bằng 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "am5sdOkPRBbn"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalizeRows\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # ord = 2 có nghĩa là trong phần này chúng ta đang input matrix 2D.\n",
    "    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n",
    "    print(\"x_norm shape: \", x_norm.shape)\n",
    "    \n",
    "    # Divide x by its norm.\n",
    "    x = x / x_norm\n",
    "    print(\"x shape: \", x.shape)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MzjMy08RBbq",
    "outputId": "9daf83dd-2a4d-4a18-e985-cf33ee467418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_norm shape:  (2, 1)\n",
      "x shape:  (2, 3)\n",
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJ8jMiDVRBbu"
   },
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "```python\n",
    "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
    "                    [0.13736056 0.82416338 0.54944226]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVwTxkWhRBbv"
   },
   "source": [
    "**Chú ý**:\n",
    "\n",
    "Trong normalizeRows(), bạn hãy thử bỏ comment `print` và in ra shapes của x_norm và x. Và chạy lại phần này, bạn sẽ thấy chúng khác nhau về số chiều (shape). Mặc dù nó khác nhau số chiều, nhưng vẫn có thể thực hiện tính toán được bình thường, đây gọi là kỹ thuật \"broadcasting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_7xtRT-RBbv"
   },
   "source": [
    "### 1.5 - Broadcasting và hàm Softmax ####\n",
    "\n",
    "\n",
    "Một khái niệm quan trọng tỏng numpy là \"broadcasting\". Nó hữu ích cho việc tính toán giữa các arrays với số chiều khác nhau. Để tìm hiểu thêm, bạn có thể đọc chi tiết tại [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "Với kỹ thuật broadcasting, chúng ta có thể dùng phép toán (a+b). Lúc này, vector b sẽ nhân bản ra để có cùng shape với a, và phép toán được thực hiện theo các thông thường.\n",
    "\n",
    "![alt text](https://imgur.com/DVKmFQW.png)\n",
    "<!-- <img src=\"images/broadcasting.png\" style=\"width:800px;height:400;\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edeh4N45RBbv"
   },
   "source": [
    "**Bài tập 5**: Thực hiện hàm `softmax` sử dụng `numpy`. Hàm `softmax` giúp chúng ta phân loại nhiều classes (multi-class). Bạn sẽ được học về softmax trong các tuần tới. \n",
    "\n",
    "**Hướng dẫn**:\n",
    "- $ \\text{Với } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{Matrix } x \\in \\mathbb{R}^{m \\times n}$, $x_{ij}$ ánh xạ tới từng phần từ ở hàng thứ $i^{th}$ cột  $j^{th}$ trong $x$, do đó chúng ta có:\n",
    "\n",
    "$ softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(hàng thứ nhất của x)}  \\\\\n",
    "    softmax\\text{(hàng thứ hai của x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(hàng cuối của x)} \\\\\n",
    "\\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MHqO1z8hRBbw"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Thực hiện hàm softmax cho mỗi hàng của input x.\n",
    "\n",
    "    Code của bạn sẽ thực hiện trên mỗi hàng của vector.\n",
    "    \n",
    "    Ma trận x có kích thước shape(m ,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Sử dụng exp() element-wise (tính toán trên từng phần tử) cho x. Dùng: np.exp(...).\n",
    "    x_exp = np.exp(x)\n",
    "    print('x_exp - shape:',x_exp.shape)\n",
    "\n",
    "    # Tạo vector x_sum với tổng theo hàng của x_exp. Use: np.sum(..., axis = 1, keepdims = True).\n",
    "    # Đầu vào của chúng ta là matrix 2 chiều nên:\n",
    "    # axis = 0 tương đương với tính tổng theo chiều dọc (cột)\n",
    "    # axis = 1 tương đương với tính tổng theo chiều ngang (hàng)\n",
    "    # keepdims = True -> giữ số chiều giống đầu vào.  \n",
    "    \n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    print('x_sum - shape', x_sum.shape)\n",
    "    \n",
    "    # Thực hiện hàm softmax(x) bằng phép chia x_exp cho x_sum. Bạn nên sử dụng kỹ thuật 'broadcasting'.\n",
    "    s = x_exp / x_sum\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_0wW6IORBbz",
    "outputId": "b390ba77-bd34-4a3a-f02f-7d060cc9b46b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_exp - shape: (2, 5)\n",
      "x_sum - shape (2, 1)\n",
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n",
      "y shape:  (2, 1)\n",
      "x / y:  [[1.         0.22222222 0.55555556 0.         0.        ]\n",
      " [1.4        1.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))\n",
    "\n",
    "y = np.array([[9], \n",
    "              [5]])\n",
    "print(\"y shape: \", y.shape)\n",
    "print('x / y: ', x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxhun3IyRBb2"
   },
   "source": [
    "**Đầu ra kỳ vọng**:\n",
    "```python\n",
    "x_exp - shape: (2, 5)\n",
    "x_sum - shape (2, 1)\n",
    "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04 1.21052389e-04]\n",
    "              [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04 8.01252314e-04]]\n",
    "(2, 1)\n",
    "x / y:  [[1.         0.22222222 0.55555556 0.         0.        ]\n",
    "         [1.4        1.         0.         0.         0.        ]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi3Mu_OrRBb2"
   },
   "source": [
    "**Chú ý**:\n",
    "\n",
    "- Nếu bạn in ra số chiều của x_exp, x_sum. Bạn sẽ thấy x_sum có shape(2, 1) trong khi đó x_exp và s có shape (2, 5). **x_exp/x_sum** hoạt động được do 'broadcasting'.\n",
    "\n",
    "\n",
    "Chúc mừng bạn đã hoàn thành phần Code bên trên về python numpy và kỹ thuật 'broadcasting'. Nó là các hàm hữu ích trong ML / DL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRaJdGMZRBb2"
   },
   "source": [
    "<font color='blue'>\n",
    "**Tổng kết một số điều bạn nên nhớ:**\n",
    "    \n",
    "- np.exp(x) hoạt động trên từng phần tử của np.array x và thực hiện hàm e mũ trên từng phần tử trong x.\n",
    "\n",
    "- Hàm sigmoid và đạo hàm của nó.\n",
    "\n",
    "- Chuyển hình ảnh 3D sang vector (image2vector), rất hữu ích trong deep learning.\n",
    "\n",
    "- Thực hiện chuyển đổi số chiều với np.reshape.\n",
    "\n",
    "- Kỹ thuật 'broadcasting' rất hiệu quả trong tính toán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy2cQzfMRBb3"
   },
   "source": [
    "## 2) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b218T8ZjRBb3"
   },
   "source": [
    "Trong ML / DL, bạn sẽ sử dụng với những bộ dữ liệu lớn. Vì thế mà chúng ta cần phải sử dụng thuật toán tối ưu để tính toán, tránh lãng phí thời gian. Bạn nên vector hoá (vectorization) sẽ giúp tính toán hiệu quả hơn. \n",
    "\n",
    "Một vài ví dụ bên dưới, bạn sẽ thấy sự khác biệt giữa phép toán dot / outer / elementwise product sử dụng vectorization và vòng lặp for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OCdpXIUARBb4",
    "outputId": "469d6ee9-a822-4cd6-e9c8-3814a1412e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.071772000000081ms\n",
      "outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- Computation time = 0.23257599999992884ms\n",
      "elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- Computation time = 0.10615799999991182ms\n",
      "gdot = [19.68682584 20.20984359 29.99622137]\n",
      " ----- Computation time = 0.18613799999989133ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHW8yuKHRBb6",
    "outputId": "30df646b-0232-4c80-fb58-b181c9a53311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- Computation time = 0.11098799999986753ms\n",
      "outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- Computation time = 0.1035480000002309ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- Computation time = 0.0568210000000402ms\n",
      "gdot = [19.68682584 20.20984359 29.99622137]\n",
      " ----- Computation time = 0.5952230000001446ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5-DcIQDRBb-"
   },
   "source": [
    "Mặc dù với bộ dữ liệu nhỏ như ví dụ ở trên, bạn mới chỉ thấy thời gian chênh lệch không đáng kể. Nhưng với dữ liệu lớn, nó sẽ rất lãng phí thời gian chạy thuật toán của bạn.\n",
    "\n",
    "\n",
    "**Lưu ý**: hàm `np.dot()` thực hiện phép nhân matrix-matrix hoặc matrix-vector. Nó khác với phép `np.multiply()` và `*`. ( `np.multiply()` và `*` có nghĩa là nhân từng phần tử hay còn gọi là phép `element-wise`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6MyzvXARBb-"
   },
   "source": [
    "### Thực hiện tính toán hàm Loss L1 và L2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vET7MpzYFvmL"
   },
   "source": [
    "**Bài tập 6**: Thực hiện tính L1 loss với numpy. \n",
    "\n",
    "**Nhắc lại**:\n",
    "- Hàm loss sẽ đánh giá hiệu suất của model. Nếu loss lớn điều đó có nghĩa là bạn giá trị dự đoán ($ \\hat{y} $) sai khác lớn so với giá trị thực tế ($y$).\n",
    "- L1 loss được định nghĩa như sau:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RdHuMMTgRBb_"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum(abs(y - yhat))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L0aVh94fRBcB",
    "outputId": "8e817dd7-9129-4b8b-964e-771b5a9caaaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l_9IIWVRBcD"
   },
   "source": [
    "**Đầu ra kỳ vọng**:\n",
    "\n",
    "```python\n",
    "L1 = 1.1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJD5sj7uRBcE"
   },
   "source": [
    "**Bài tập 7**: Thực hiện tính toán hàm L2 loss. \n",
    "\n",
    "Có một cách khác để thực hiện hàm L2 loss là bạn sử dụng np.dot(). \n",
    "\n",
    "Với: $x = [x_1, x_2, ..., x_n]$, thì `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- Công thức hàm L2 Loss: $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "HtoqaySWRBcE"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    # Using function np.dot() useful.\n",
    "    loss = np.dot(y - yhat, y - yhat)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNFLl2-MRBcG",
    "outputId": "6ebf25b2-88cc-43bf-c652-f457423bf684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42CK2kT8RBcJ"
   },
   "source": [
    "**Đầu ra kỳ vọng**: \n",
    "```python\n",
    "L2 = 0.43\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb0nDn6jRBcJ"
   },
   "source": [
    "Chúc mừng bạn đã hoàn thành bài tập này. Mong rằng nó sẽ giúp ích cho bạn !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j75-5pI5RBcK"
   },
   "source": [
    "<font color='blue'>\n",
    "**Những điều bạn cần nhớ:**\n",
    "    \n",
    "- Vectorization rất quan trong trong Deep Learning. Nó giúp tính toán hiệu quả hơn. \n",
    "\n",
    "- Nhớ và phân biệt hàm loss: l1 và l2.\n",
    "\n",
    "- Các hàm: np.sum, np.dot, np.multiply, np.maximun, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc5lUFVIRBcK"
   },
   "source": [
    "### Tài liệu tham khảo\n",
    "[1] [Deep Learning - Coursera](https://coursera.org)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Đạt Nguyễn - [ProtonX] Lập trình Python trong Machine Learning / Deep Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
